"""
src/hitl/hitl_manager.py
Manages the Human-in-the-Loop (HITL) review process for ETL task definitions,
including iterative feedback and modification.
"""

# General Imports
import json
from typing import List, Callable

# AutoGen Imports - for agent management
from autogen import UserProxyAgent, AssistantAgent

# LangChain Imports - for LLM and prompt handling
from langchain.prompts import PromptTemplate
from langchain_core.exceptions import OutputParserException
from langchain_core.documents import Document # Import Document for type hinting
from langchain_core.output_parsers import PydanticOutputParser

# Project Imports - for schema and utility functions
from src.core.schemas import ETLTaskDefinition
from src.core.vector_db_manager import VectorDBManager # Import for type hinting (db_manager_approved_tasks)
from src.utils.utils import extract_json_from_llm_output # Import the utility function

def initiate_hitl_review(
    task_json: dict,
    parser_agent_instance: any, # Using 'any' to avoid circular import type hint issues
    full_context_docs: List[Document],
    original_request: str,
    db_manager_approved_tasks: VectorDBManager,
    ingest_approved_etl_task_func: Callable, # Function to ingest approved tasks, passed as argument
    debug_mode: bool = False # Debug flag passed as argument
) -> bool:
    """
    Manages the Human-in-the-Loop (HITL) review process with iterative feedback and modification.
    :param task_json: The structured ETL task definition (dictionary) to be reviewed.
    :param parser_agent_instance: The ParserAgent instance, used to access its LLM for modifications.
    :param full_context_docs: List of LangChain Document objects containing combined dataset and approved task metadata for LLM grounding.
    :param original_request: The original natural language request for storing with approved tasks.
    :param db_manager_approved_tasks: The VectorDBManager instance for the approved tasks index.
    :param ingest_approved_etl_task_func: A callable function (from utils) to ingest approved ETL tasks.
    :param debug_mode: A boolean flag to enable/disable debug logging.
    :return: True if the task is approved, False otherwise.
    """
    human_data_engineer = UserProxyAgent(
        name="Data_Engineer",
        human_input_mode="ALWAYS", # Always prompt for human input
        max_consecutive_auto_reply=0, # Never auto-reply for this agent in this setup
        is_termination_msg=lambda x: x.get("content", "").strip().lower() in ["approve", "approved", "deny", "denied"],
        system_message="You are a data engineer responsible for reviewing and approving ETL pipeline task definitions generated by an AI agent. Review the provided JSON carefully. Respond with 'approve', 'deny', or provide feedback for modification.",
        code_execution_config={"use_docker": False}
    )

    # Using AssistantAgent as requested, configured not to auto-reply.
    parser_messenger = AssistantAgent(
        name="Parser_Messenger",
        system_message="You present the ETL task definition and facilitate human feedback. You do not generate responses using an LLM in this chat.",
        llm_config=False, # Disables its ability to use an LLM for generating its own responses
        max_consecutive_auto_reply=0, # Ensure no auto-replies
    )

    current_task_json = task_json.copy()
    feedback_history = [] # Initialize list to store modification feedback

    # Format the full context for the modification prompt
    full_context_string_for_llm = ""
    if full_context_docs:
        full_context_string_for_llm = "Available Context (Dataset and Approved Task Details):\n"
        for i, doc in enumerate(full_context_docs):
            full_context_string_for_llm += f"  - {doc.page_content}"
            if doc.metadata:
                full_context_string_for_llm += f" (Metadata: {json.dumps(doc.metadata)})"
            full_context_string_for_llm += "\n"
    else:
        full_context_string_for_llm = "No specific context available for features."

    # For modification loop, use full context
    current_context_for_modification_llm = full_context_string_for_llm
    if debug_mode: # Use debug_mode parameter
        current_context_for_modification_llm = "Minimal context for debugging modification. User feedback: " + original_request[:100] + "..."


    while True: # Loop for iterative feedback
        print("\n--- Current Task Definition for Review ---")
        print(json.dumps(current_task_json, indent=2))
        print("----------------------------------------\n")

        # The parser_messenger initiates the chat with human_data_engineer.
        chat_result = parser_messenger.initiate_chat(
            human_data_engineer,
            message=f"Please review the following ETL task definition. You can '**approve**', '**deny**', or provide **feedback for modification** (e.g., 'add output_location', 'change join type to inner for bureau', 'add features from available dataset context').\n\n```json\n{json.dumps(current_task_json, indent=2)}\n```\n\n{full_context_string_for_llm}\n\nWhat is your decision or feedback?",
        )

        if chat_result.chat_history:
            human_response_raw = chat_result.chat_history[-1].get("content", "")
            human_response = human_response_raw.strip().lower()
        else:
            print("Error: chat_history is empty, cannot retrieve human response. Terminating review.")
            return False


        if human_response in ["approve", "approved"]:
            print("\n--- Task Approved by Data Engineer ---")
            # Ingest the approved task using the passed function
            ingest_approved_etl_task_func(db_manager_approved_tasks, current_task_json, original_request, modification_feedback_history=feedback_history)
            return True
        elif human_response in ["deny", "denied"]:
            print("\n--- Task Denied by Data Engineer ---")
            if human_response_raw.strip().lower() not in ["deny", "denied"]:
                print(f"Reason for denial: {human_response_raw.strip()}")
            return False
        else:
            # Human provided feedback for modification - add to history
            feedback_history.append(human_response_raw.strip())
            print(f"\n--- Data Engineer requested modification: '{human_response_raw.strip()}' ---")
            print("AI is updating the task definition based on feedback...")

            # Define the prompt for LLM modification
            modification_prompt = PromptTemplate(
                template="""You are an AI assistant specialized in modifying ETL task definitions.
                You will be provided with the current ETL task definition in JSON format and specific feedback from a data engineer.
                Your goal is to adjust the JSON task definition based on the feedback.
                Crucially, you must **preserve all existing fields** unless the feedback explicitly instructs you to remove or change them.
                If feedback relates to a specific field, modify only that field. If it's about adding information, add it.
                Specifically, when asked to add or modify `features` under `scoring_model`, **select relevant column names ONLY from the provided 'Available Context' (Dataset or Approved Task Examples)**. Do not invent feature names. If no relevant columns are provided in the context, state that you cannot add specific features.

                You must output ONLY the updated JSON, strictly adhering to the Pydantic schema for ETLTaskDefinition.
                Ensure all **required fields** (like `pipeline_name`, `main_goal`) remain present and valid in the output.
                Do NOT include any conversational text, markdown code block (like ```json), or explanations outside the JSON.

                Pydantic Schema:
                {format_instructions}

                Current ETL Task Definition:
                {current_json}

                Data Engineer's Feedback:
                {feedback}

                Available Context:
                {dataset_context}

                Updated JSON Output:
                """,
                input_variables=["current_json", "feedback", "dataset_context"], # Renamed to dataset_context for consistency with prompt
                partial_variables={"format_instructions": PydanticOutputParser(pydantic_object=ETLTaskDefinition).get_format_instructions()},
            )

            # Explicitly use the main parser_agent_instance's LLM for modification
            modification_chain = modification_prompt | parser_agent_instance.llm
            
            print("DEBUG: Attempting to invoke LLM chain for modification...")
            try:
                raw_llm_string_output_mod = modification_chain.invoke({
                    "current_json": json.dumps(current_task_json, indent=2),
                    "feedback": human_response_raw.strip(),
                    "dataset_context": current_context_for_modification_llm
                }, config={"timeout": 300.0})
                print("DEBUG: LLM chain invocation for modification completed.")

                # Extract clean JSON string from LLM's raw output for modification
                cleaned_json_string_mod = extract_json_from_llm_output(raw_llm_string_output_mod)

                if debug_mode: # Use debug_mode parameter
                    print("\n--- DEBUG: Raw LLM Output (Modification, after stripping fences) ---")
                    print(cleaned_json_string_mod)
                    print("----------------------------------------------------------------------")
                
                # Attempt to parse the cleaned string with PydanticOutputParser
                updated_task_pydantic = parser_agent_instance.parser.parse(cleaned_json_string_mod)
                current_task_json = updated_task_pydantic.model_dump()
                print("Task definition updated. Presenting for re-review.")
            except TimeoutError:
                print(f"Error: LLM modification timed out after 300 seconds.")
                print("Failed to apply modification due to timeout. Please try again or provide more precise feedback.")
            except OutputParserException as e:
                print(f"Error parsing LLM output during modification: {e}")
                print(f"Raw LLM output (for debugging): {cleaned_json_string_mod if 'cleaned_json_string_mod' in locals() else 'Not available'}")
                print("Failed to apply modification. Please try again or provide more precise feedback.")
            except json.JSONDecodeError as e:
                print(f"Error: Cleaned LLM modification output is not valid JSON: {e}")
                print("Failed to apply modification. Cleaned LLM output is not valid JSON, cannot proceed.")
            except Exception as e:
                print(f"An unexpected error occurred during modification: {e}")
                print("Failed to apply modification. Please try again.")

    return False
